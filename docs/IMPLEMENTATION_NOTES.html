<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>üî¨ Implementation Deep Dive - MPMC Queue</title>
    <link rel="stylesheet" href="shared-styles.css">

</head>
<body>
    <div class="container">
        <div class="header">
            <a href="https://github.com/abbychau/mpmc-std" class="github-corner" target="_blank" rel="noopener noreferrer">
                <svg xmlns="http://www.w3.org/2000/svg" width="80" height="80" viewBox="0 0 250 250" fill="#151513" style="position: absolute; top: 0; right: 0">
                <path d="M0 0l115 115h15l12 27 108 108V0z" fill="#000"/>
                <path class="octo-arm" d="M128 109c-15-9-9-19-9-19 3-7 2-11 2-11-1-7 3-2 3-2 4 5 2 11 2 11-3 10 5 15 9 16" style="-webkit-transform-origin: 130px 106px; transform-origin: 130px 106px"/>
                <path class="octo-body" d="M115 115s4 2 5 0l14-14c3-2 6-3 8-3-8-11-15-24 2-41 5-5 10-7 16-7 1-2 3-7 12-11 0 0 5 3 7 16 4 2 8 5 12 9s7 8 9 12c14 3 17 7 17 7-4 8-9 11-11 11 0 6-2 11-7 16-16 16-30 10-41 2 0 3-1 7-5 11l-12 11c-1 1 1 5 1 5z"/>
                </svg>
            </a>
            <h1>üî¨ Implementation Deep Dive</h1>
            <p>Technical details and performance engineering</p>
        </div>
        
        <div class="content">
            <div style="text-align: center; margin-bottom: 2rem;">
                <a href="index.html" class="nav-back">‚Üê Back to Documentation Index</a>
            </div>
<h1>Implementation Details</h1>
<p>This document explains the key implementation decisions and technical details of the lockless MPMC queue.</p>
<h2>Core Algorithm</h2>
<h3>Sequence-Based Coordination</h3>
<p>The algorithm uses sequence numbers instead of flags for coordination:</p>
<br>
<pre><code class="language-rust">
struct Slot&lt;T&gt; {
    sequence: AtomicUsize,              // Single coordination point
    data: UnsafeCell&lt;MaybeUninit&lt;T&gt;&gt;,   // Raw storage
}
</code></pre>
<br>
<p><strong>Benefits:</strong></p>
<ul>
<li>ABA immunity: sequences always advance</li>
<li>Single atomic operation for state check</li>
<li>Natural ordering guarantees</li>
<li>Wait-free progress bounds</li>
</ul>
<h3>Memory Ordering</h3>
<p>Careful memory ordering ensures correctness:</p>
<ul>
<li><strong>Relaxed</strong>: Position loads (just need current value)</li>
<li><strong>Acquire</strong>: Sequence loads (see previous writes)</li>
<li><strong>Release</strong>: Sequence stores (make writes visible)</li>
</ul>
<br>
<pre><code class="language-rust">
// Producer stores data BEFORE updating sequence
unsafe { (*slot.data.get()).write(item); }
slot.sequence.store(new_seq, Ordering::Release);
</code></pre>
<h3>ABA Problem Prevention</h3>
<p>Sequence numbers prevent the ABA problem by always advancing:</p>
<br>
<pre><code>
Slot 0: 0 ‚Üí 1 ‚Üí 8 ‚Üí 9 ‚Üí 16 ‚Üí 17 ‚Üí ...
        ‚Üë   ‚Üë   ‚Üë   ‚Üë    ‚Üë    ‚Üë
       Init Prod Cons Prod Cons Prod
</code></pre>
<br>
<p>Even if the same logical state occurs, the sequence number is different.</p>
<h2>Power-of-2 Optimization</h2>
<p>Capacity is rounded to power-of-2 for fast indexing:</p>
<br>
<pre><code class="language-rust">
// Instead of expensive modulo:
position % capacity  // ~20-40 cycles
// Use fast bitwise AND:
position &amp; mask      // ~1 cycle
// Example: capacity=1024, mask=1023
// 5000 % 1024 = 904  (slow)
// 5000 &amp; 1023 = 904  (fast)
</code></pre>
<br>
<p>Capacity is automatically rounded up to the next power-of-2.</p>
<h2>Cache-Line Alignment</h2>
<p>Prevents false sharing by separating producer and consumer positions:</p>
<br>
<pre><code class="language-rust">
#[repr(align(64))]  // 64-byte cache line alignment
struct ProducerPos {
    head: AtomicUsize,
    // 56 bytes padding
}
#[repr(align(64))]
struct ConsumerPos {
    tail: AtomicUsize, 
    // 56 bytes padding
}
</code></pre>
<br>
<p><strong>Benefit</strong>: ~40% performance improvement by eliminating cache invalidations between cores.</p>
<h2>Atomic Operations</h2>
<h3>Compare-Exchange-Weak</h3>
<p>Uses <code>compare<em>exchange</em>weak</code> instead of strong version:</p>
<ul>
<li>More efficient on ARM/PowerPC architectures</li>
<li>Allows spurious failures (acceptable since we're in retry loops)</li>
<li>Better power consumption</li>
</ul>
<h3>Memory Ordering Strategy</h3>
<ul>
<li><strong>Relaxed</strong>: Position loads (no synchronization needed)</li>
<li><strong>Acquire</strong>: Sequence loads (see previous writes)</li>
<li><strong>Release</strong>: Sequence stores (make writes visible)</li>
<li><strong>Avoids SeqCst</strong>: Too expensive for this use case</li>
</ul>
<h2>Memory Safety</h2>
<p>Achieves memory safety without garbage collection:</p>
<br>
<pre><code class="language-rust">
struct Slot&lt;T&gt; {
    sequence: AtomicUsize,              // Coordination
    data: UnsafeCell&lt;MaybeUninit&lt;T&gt;&gt;,   // Safe uninitialized storage
}
</code></pre>
<br>
<p><strong>Safety invariants:</strong></p>
<ol>
<li>Data written only when sequence == expected</li>
<li>Data read only when sequence == expected + 1</li>
<li>Sequence coordination prevents access races</li>
<li>Proper cleanup in Drop implementation</li>
</ol>
<br>
<p><strong>Send/Sync traits</strong>: Safely implemented using sequence coordination to prevent data races.</p>
<h2>Performance Optimizations</h2>
<h3>Branch Prediction</h3>
<p>Code structure favors common case (successful operations) to help CPU branch predictor.</p>
<h3>CPU Cache Utilization</h3>
<ul>
<li>Sequential ring buffer access patterns enable prefetching</li>
<li>Temporal locality: threads access consecutive slots</li>
<li>Spatial locality: 64-byte alignment fits cache boundaries</li>
<li>Each position gets own cache line to prevent false sharing</li>
</ul>
<h2>Common Pitfalls</h2>
<h3>Sequence Number Overflow</h3>
<p>Wrapping arithmetic is intentional and safe - maintains ordering even when numbers wrap around.</p>
<h3>Memory Ordering</h3>
<p>Critical to use <code>Release</code> ordering when storing sequences to make data writes visible.</p>
<h3>Capacity Requirements</h3>
<p>Capacity is automatically rounded up to next power-of-2 for optimal performance.</p>
<h2>Performance Tips</h2>
<h3>Profiling</h3>
<ul>
<li>Monitor cache miss rate (<5% is good)</li>
<li>Check branch prediction accuracy (>95% is good)</li>
<li>Watch cycles per instruction (<2.0 is good)</li>
</ul>
<h3>Common Issues</h3>
<ul>
<li><strong>Too small capacity</strong>: Increase size to reduce contention</li>
<li><strong>Too many threads</strong>: Consider multiple queues with >8 threads</li>
<li><strong>Large items</strong>: Use <code>Arc<T></code> or <code>Box<T></code> for indirect storage</li>
</ul>
<h3>Debugging</h3>
<pre><code class="language-rust">
// Queue provides introspection methods
queue.capacity();  // Power-of-2 capacity
queue.len();       // Approximate current length
queue.is_empty();  // Snapshot view
queue.is_full();   // Snapshot view
</code></pre>
<h2>SIMD Optimizations (Nightly Rust)</h2>
<p>The MPMC queue includes SIMD (Single Instruction Multiple Data) optimizations for <code>u64</code> data types when compiled with nightly Rust and the <code>simd</code> feature.</p>
<h3>SIMD Architecture</h3>
<pre><code class="language-rust">
use std::simd::{u64x4, SimdPartialEq};
struct SimdMpmcQueue&lt;T&gt; {
    buffer: Box&lt;[SimdSlot&lt;T&gt;]&gt;,
    simd_batch_size: usize,    // 4 for u64x4 SIMD width
    // ... other fields
}
</code></pre>
<h3>Vectorized Operations</h3>
<h4>Batch Sequence Checking</h4>
<p>The SIMD implementation can check 4 slot sequences simultaneously:</p>
<br>
<pre><code class="language-rust">
// Load 4 sequence numbers using SIMD
let sequences = unsafe { self.load_sequences_simd(head, 4) };
let expected = self.generate_expected_sequences_simd(head, 4);
// Compare all 4 sequences at once
let mask = sequences.simd_eq(expected);
if mask.all() {
    // All 4 slots are available
}
</code></pre>
<h4>Batch Operations</h4>
<p>Process 4 u64 elements in a single operation:</p>
<br>
<pre><code class="language-rust">
// Send batch of 4 u64 values
let batch = vec![1u64, 2u64, 3u64, 4u64];
producer.send_batch(&amp;batch)?;
// Receive batch of 4 u64 values
let mut buffer = vec![0u64; 4];
let count = consumer.recv_batch(&amp;mut buffer);
</code></pre>
<h3>SIMD Performance Characteristics</h3>
<p><strong>Throughput Improvements:</strong></p>
<ul>
<li><strong>High Contention</strong>: Up to 1.8x speedup with 4+ thread pairs</li>
<li><strong>Single Thread</strong>: 10-30% improvement for individual operations</li>
<li><strong>Optimal</strong>: u64 data in 4-element aligned batches</li>
</ul>
<br>
<p><strong>Memory Requirements:</strong></p>
<ul>
<li>Minimum capacity: 16 elements (2x SIMD width)</li>
<li>Still power-of-2 rounded for efficient masking</li>
<li>Cache-line aligned SIMD slot structures</li>
</ul>
<h3>SIMD Algorithm Details</h3>
<h4>Vectorized Availability Check</h4>
<p>Instead of checking each slot individually:</p>
<br>
<pre><code class="language-rust">
// Traditional approach (4 separate checks)
for i in 0..4 {
    let slot = &amp;buffer[(head + i) &amp; mask];
    if slot.sequence.load(Acquire) == head + i {
        // Slot available
    }
}
// SIMD approach (single vectorized check)
let sequences = u64x4::from_array([
    slot0.sequence.load(Acquire) as u64,
    slot1.sequence.load(Acquire) as u64,
    slot2.sequence.load(Acquire) as u64,
    slot3.sequence.load(Acquire) as u64,
]);
let expected = u64x4::from_array([head, head+1, head+2, head+3]);
let all_ready = sequences.simd_eq(expected).all();
</code></pre>
<h4>Batch Memory Operations</h4>
<p>Leverages CPU's vectorized memory instructions:</p>
<br>
<pre><code class="language-rust">
// Store 4 u64 values efficiently
let simd_data = u64x4::from_slice(items);
// Hardware can optimize this to vectorized stores
</code></pre>
<h3>SIMD Usage Guidelines</h3>
<p><strong>Best Performance:</strong></p>
<ul>
<li>u64 numeric data types</li>
<li>Batch sizes of exactly 4 elements</li>
<li>High-contention multi-threaded scenarios</li>
<li>CPU with AVX2+ support (x86-64)</li>
</ul>
<br>
<p><strong>When to Use Regular Queue:</strong></p>
<ul>
<li>Mixed data types or sizes</li>
<li>Variable batch sizes < 4</li>
<li>Low-contention scenarios</li>
<li>Stable Rust requirement</li>
</ul>
<br>
<p><strong>Hybrid Strategy:</strong></p>
<pre><code class="language-rust">
// SIMD queue provides both interfaces
if data.len() &gt;= 4 &amp;&amp; data.len() % 4 == 0 {
    // Use SIMD batch operations
    producer.send_batch(&amp;data[..4])?;
} else {
    // Fall back to single operations
    producer.send(data[0])?;
}
</code></pre>
<h3>SIMD Compilation Requirements</h3>
<p><strong>Toolchain:</strong></p>
<pre><code class="language-bash">
rustup default nightly
</code></pre>
<br>
<p><strong>Features:</strong></p>
<pre><code class="language-toml">
[features]
simd = []
default = [&quot;simd&quot;]
</code></pre>
<br>
<p><strong>Build Command:</strong></p>
<pre><code class="language-bash">
cargo build --features simd
</code></pre>
<br>
<p>The SIMD implementation maintains all wait-free, lockless guarantees while providing significant performance improvements for suitable workloads.</p>
        </div>
        
        <div class="nav-grid">
            <div class="nav-card">
                <p>Explore the complete technical documentation suite for the MPMC Queue implementation.</p>
                <div style="display: flex; gap: 1rem; margin-top: 1.5rem;">
                    <a href="index.html" style="flex: 1; text-align: center;">Documentation Index</a>
                    <a href="COMPARATIVE_ANALYSIS.html" style="flex: 1; text-align: center;">Comparative Analysis</a>
                </div>
            </div>
        </div>
    </div>

    <div class="footer">
        <div class="footer-brand">MPMC Queue - High-Performance Lockless Data Structure</div>
        <p>Built with Rust ‚Ä¢ Benchmarked with Criterion.rs ‚Ä¢ Optimized for Modern Hardware</p>
        <p style="font-size: 0.9rem; margin-top: 1rem;">
            Research-grade implementation combining Michael & Scott, LMAX Disruptor, and modern optimization techniques
        </p>
    </div>
</body>
</html>
