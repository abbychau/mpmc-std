<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Implementation Notes - MPMC Queue</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            background: #f8f9fa;
        }
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem;
            border-radius: 10px;
            margin-bottom: 2rem;
            text-align: center;
        }
        .content {
            background: white;
            padding: 2rem;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        pre {
            background: #f4f4f4;
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
            border-left: 4px solid #667eea;
        }
        code {
            background: #f4f4f4;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
        }
        .nav-back {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 0.5rem 1rem;
            text-decoration: none;
            border-radius: 5px;
            margin-bottom: 1rem;
        }
        .nav-back:hover {
            background: #5a67d8;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ”¬ Implementation Deep Dive</h1>
        <p>Technical details and performance engineering</p>
    </div>
    
    <a href="index.html" class="nav-back">â† Back to Documentation Index</a>
    
    <div class="content">


<h1>ğŸ”¬ Implementation Deep Dive



This document explains the low-level implementation details, design decisions, and technical rationale behind the lockless MPMC queue.





<h2>ğŸ§  Core Algorithm Design





<h3>Why Sequence-Based Coordination?



The algorithm uses **sequence numbers** instead of traditional approaches for several critical reasons:




<pre><code>rust

// âŒ Traditional flag-based approach (problematic):

struct TraditionalSlot<T> {

    filled: AtomicBool,        // Separate flag

    data: UnsafeCell<Option<T>>, // Separate data

}



// Problems:

// 1. ABA problem: flag can be reused

// 2. Two atomic operations needed

// 3. Race conditions between flag and data

// 4. Memory ordering complexities



// âœ… Our sequence-based approach (superior):

struct Slot<T> {

    sequence: AtomicUsize,              // Single coordination point

    data: UnsafeCell<MaybeUninit<T>>,   // Raw storage

}



// Benefits:

// 1. ABA immunity: sequences always advance

// 2. Single atomic operation for state check

// 3. Natural ordering guarantees

// 4. Wait-free progress bounds


<pre><code>





<h3>Memory Ordering Rationale




<pre><code>rust

// Producer sequence (carefully chosen orderings):

pub fn send(&self, item: T) -> Result<(), T> {

    loop {

        let head = self.producer_pos.head.load(Ordering::Relaxed);

        //                                    ^^^^^^^^

        // Relaxed: No synchronization needed, just get current position

        

        let seq = slot.sequence.load(Ordering::Acquire);

        //                           ^^^^^^^^^^^^^^^^

        // Acquire: Ensure we see all writes that happened-before

        // the sequence store from consumer

        

        match self.producer_pos.head.compare_exchange_weak(

            head, head.wrapping_add(1),

            Ordering::Relaxed,  // Success: position update doesn't need sync

            Ordering::Relaxed,  // Failure: just retry, no sync needed

        ) {

            Ok(_) => {

                // Store data BEFORE updating sequence (critical ordering!)

                unsafe { (*slot.data.get()).write(item); }

                

                slot.sequence.store(

                    expected_seq.wrapping_add(1), 

                    Ordering::Release  // Release: Make our data write visible

                );                    // to consumers before they see new sequence

                return Ok(());

            }

        }

    }

}


<pre><code>





<h3>ABA Problem Prevention




<pre><code>

Classic ABA Problem (avoided by our design):



Thread 1: Read value A from location X

Thread 2: Change X from A to B  

Thread 3: Change X from B back to A

Thread 1: CAS succeeds thinking nothing changed!



Our Solution - Monotonic Sequences:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚ Sequence numbers NEVER repeat for the same slot:       â”‚

â”‚                                                         â”‚

â”‚ Slot 0: 0 â†’ 1 â†’ 8 â†’ 9 â†’ 16 â†’ 17 â†’ 24 â†’ 25 â†’ ...      â”‚

â”‚         â†‘   â†‘   â†‘   â†‘    â†‘    â†‘     â†‘    â†‘             â”‚

â”‚         â”‚   â”‚   â”‚   â”‚    â”‚    â”‚     â”‚    â”‚             â”‚

â”‚      Initâ”‚Proâ”‚Conâ”‚ Pro â”‚ Conâ”‚ Pro â”‚Con â”‚ Pro           â”‚

â”‚          â”‚ducâ”‚sumâ”‚duc  â”‚sum â”‚duc  â”‚sum â”‚duc            â”‚

â”‚          â”‚er â”‚er â”‚er   â”‚er  â”‚er   â”‚er  â”‚er             â”‚

â”‚                                                         â”‚

â”‚ Even if same logical state, sequence differs!          â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


<pre><code>





<h2>ğŸ¯ Power-of-2 Capacity Optimization





<h3>Why Power-of-2 Matters




<pre><code>rust

// âŒ Expensive modulo operation:

fn slow_index(position: usize, capacity: usize) -> usize {

    position % capacity  // Division instruction ~20-40 cycles

}



// âœ… Fast bitwise AND operation:  

fn fast_index(position: usize, mask: usize) -> usize {

    position & mask      // Single AND instruction ~1 cycle

}



// Compiler optimization example:

// Input: capacity = 1024

let mask = capacity - 1;  // mask = 1023 = 0b1111111111



// position = 5000

// 5000 % 1024     = 904   (slow division)

// 5000 & 1023     = 904   (fast bitwise AND)



// Binary demonstration:

// 5000 = 0b1001110001000

// 1023 = 0b0001111111111  

// AND  = 0b0001110001000 = 904


<pre><code>





<h3>Automatic Power-of-2 Rounding




<pre><code>rust

impl<T: Send> MpmcQueue<T> {

    pub fn new(capacity: usize) -> Self {

        assert!(capacity > 0, "Capacity must be greater than 0");

        

        // Round up to next power of 2 for optimal performance

        let capacity = capacity.next_power_of_two();

        let mask = capacity - 1;  // Efficient modulo mask

        

        // Examples:

        // Input: 100  â†’ capacity: 128,  mask: 127

        // Input: 256  â†’ capacity: 256,  mask: 255  

        // Input: 1000 â†’ capacity: 1024, mask: 1023

    }

}


<pre><code>





<h2>ğŸ—ï¸ Memory Layout Engineering





<h3>Cache-Line Alignment Deep Dive




<pre><code>

Problem: False Sharing Performance Kill



Before optimization (BAD):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â† 64-byte cache line

â”‚ producer_head â”‚ consumer_tail â”‚ other_field â”‚ another_field â”‚

â”‚   (8 bytes)   â”‚   (8 bytes)   â”‚ (8 bytes)   â”‚  (40 bytes)   â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

       â†‘                â†‘

   CPU Core 1      CPU Core 2

   

When Core 1 updates producer_head:

1. Entire cache line becomes "dirty"

2. Core 2's cache is invalidated  

3. Core 2 must reload entire cache line

4. ~100ns penalty + memory bus contention



After optimization (GOOD):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â† Cache Line 1

â”‚             producer_head                                   â”‚

â”‚ (8 bytes + 56 bytes padding)                               â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

       â†‘

   CPU Core 1



â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â† Cache Line 2

â”‚             consumer_tail                                   â”‚  

â”‚ (8 bytes + 56 bytes padding)                               â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

       â†‘

   CPU Core 2



Now updates are independent: ~40% performance improvement!


<pre><code>





<h3>Slot Memory Layout




<pre><code>rust

#[repr(align(64))]  // Force 64-byte alignment

struct Slot<T> {

    sequence: AtomicUsize,           // 8 bytes

    data: UnsafeCell<MaybeUninit<T>>, // sizeof(T) bytes

    // Implicit padding to 64-byte boundary

}



// Memory layout for Slot<u64>:

// Offset 0-7:   sequence (AtomicUsize)

// Offset 8-15:  data (u64)  

// Offset 16-63: padding (48 bytes)

//

// Each slot gets its own cache line when possible,

// minimizing contention between adjacent slots


<pre><code>





<h2>âš¡ Atomic Operations Strategy





<h3>Compare-Exchange-Weak vs Strong




<pre><code>rust

// Why we use compare_exchange_weak:



// âŒ compare_exchange (strong) - not optimal:

match atomic.compare_exchange(

    expected, new,

    Ordering::Release, Ordering::Relaxed

) {

    Ok(prev) => { /* always succeeds if values match */ },

    Err(prev) => { /* retry needed */ },

}



// âœ… compare_exchange_weak (better) - our choice:

match atomic.compare_exchange_weak(

    expected, new, 

    Ordering::Release, Ordering::Relaxed

) {

    Ok(prev) => { /* success */ },

    Err(prev) => { /* may fail spuriously - but that's OK! */ },

}



// Benefits of weak version:

// 1. More efficient on ARM/PowerPC (no retry loop in hardware)

// 2. Allows LL/SC architectures to be more efficient

// 3. We're already in a retry loop, so spurious failures are fine

// 4. Better power consumption on mobile processors


<pre><code>





<h3>Memory Ordering Minimization




<pre><code>rust

// Our ordering strategy (carefully optimized):



// Relaxed operations (cheapest):

let head = self.producer_pos.head.load(Ordering::Relaxed);

// Just need the value, no synchronization required



// Acquire operations (moderate cost):  

let seq = slot.sequence.load(Ordering::Acquire);

// Need to see all previous writes that led to this sequence value



// Release operations (moderate cost):

slot.sequence.store(new_seq, Ordering::Release);

// Make our data write visible before consumers see new sequence



// We avoid:

// - SeqCst: Too expensive, unnecessary global ordering

// - AcqRel: Overkill for our use case

// - Fencing: Compiler barriers sufficient


<pre><code>





<h2>ğŸ”’ Safety Guarantees





<h3>Memory Safety Without Garbage Collection




<pre><code>rust

// Challenge: How to safely manage T without GC?



// âŒ Naive approach (unsafe):

struct BadSlot<T> {

    data: *mut T,  // Raw pointer - can dangle!

}



// âœ… Our approach (safe + efficient):

struct Slot<T> {

    sequence: AtomicUsize,              // Coordination

    data: UnsafeCell<MaybeUninit<T>>,   // Safe uninitialized storage

}



// Safety invariants we maintain:

// 1. Data is only written when sequence == expected

// 2. Data is only read when sequence == expected + 1  

// 3. Data is properly dropped in queue destructor

// 4. No access races due to sequence coordination



impl<T> Drop for MpmcQueue<T> {

    fn drop(&mut self) {

        // Safely drain all remaining items

        while !self.is_empty_unchecked() {

            let tail = self.consumer_pos.tail.load(Ordering::Relaxed);

            let slot = &self.buffer[tail & self.mask];

            let seq = slot.sequence.load(Ordering::Acquire);

            

            if seq == tail.wrapping_add(1) {

                // Safe to read and drop data

                if self.consumer_pos.tail.compare_exchange_weak(

                    tail, tail.wrapping_add(1),

                    Ordering::Relaxed, Ordering::Relaxed,

                ).is_ok() {

                    unsafe {

                        (*slot.data.get()).assume_init_drop();

                    }

                    slot.sequence.store(

                        tail.wrapping_add(self.capacity),

                        Ordering::Release,

                    );

                }

            } else {

                break; // Inconsistent state, stop

            }

        }

    }

}


<pre><code>





<h3>Send/Sync Trait Implementation




<pre><code>rust

// Why our unsafe impls are actually safe:



unsafe impl<T: Send> Send for MpmcQueue<T> {}

// Safe because:

// - All internal state is properly synchronized with atomics

// - T: Send means individual items can be sent between threads

// - Our algorithm ensures no data races



unsafe impl<T: Send> Sync for MpmcQueue<T> {}  

// Safe because:

// - Multiple threads can safely access queue concurrently

// - All operations are atomic or properly synchronized

// - No shared mutable state without synchronization

// - Sequence numbers prevent all race conditions


<pre><code>





<h2>ğŸ“Š Performance Engineering





<h3>Branch Prediction Optimization




<pre><code>rust

// Our code is designed to be branch-predictor friendly:



pub fn send(&self, item: T) -> Result<(), T> {

    loop {  // Hot loop - CPU will predict this well

        let head = self.producer_pos.head.load(Ordering::Relaxed);

        let slot = &self.buffer[head & self.mask];

        let seq = slot.sequence.load(Ordering::Acquire);

        let expected_seq = head;

        

        // Common case first (branch predictor learns this):

        match seq.cmp(&expected_seq) {

            std::cmp::Ordering::Equal => {

                // MOST COMMON PATH - predictor learns this

                match self.producer_pos.head.compare_exchange_weak(

                    head, head.wrapping_add(1),

                    Ordering::Relaxed, Ordering::Relaxed,

                ) {

                    Ok(_) => {

                        // SUCCESS PATH - also common

                        unsafe { (*slot.data.get()).write(item); }

                        slot.sequence.store(

                            expected_seq.wrapping_add(1), 

                            Ordering::Release

                        );

                        return Ok(());

                    }

                    Err(_) => continue, // Retry - less common

                }

            }

            std::cmp::Ordering::Less => {

                // Queue full check - uncommon in well-sized queues

                let tail = self.consumer_pos.tail.load(Ordering::Acquire);

                if head.wrapping_sub(tail) >= self.capacity {

                    return Err(item);

                }

                continue;

            }

            std::cmp::Ordering::Greater => {

                // Race condition - very uncommon

                continue;

            }

        }

    }

}


<pre><code>





<h3>CPU Cache Optimization




<pre><code>rust

// Prefetching strategy (implicit in our design):



// 1. Sequential access patterns for ring buffer

let slot = &self.buffer[position & self.mask];

// CPU prefetcher sees this pattern and loads nearby slots



// 2. Temporal locality  

// Producers tend to access consecutive slots

// Consumers tend to access consecutive slots

// CPU caches exploit this



// 3. Spatial locality

// 64-byte aligned slots fit cache line boundaries

// Related data accessed together



// 4. Cache line utilization

#[repr(align(64))]

struct ProducerPos {

    head: AtomicUsize,  // 8 bytes used

    // 56 bytes padding - prevents false sharing

}


<pre><code>





<h2>ğŸ› Common Pitfalls and Solutions





<h3>Pitfall 1: Sequence Number Overflow




<pre><code>rust

// Problem: What happens when sequence numbers wrap?

// Solution: Wrapping arithmetic is intentional and safe!



let seq = slot.sequence.load(Ordering::Acquire);

let expected = position.wrapping_add(1);  // Handles overflow correctly



// Example with u8 for clarity (we use usize):

// position = 255, expected = 256 â†’ wraps to 0  

// This is correct behavior and maintains ordering


<pre><code>





<h3>Pitfall 2: Memory Ordering Mistakes




<pre><code>rust

// âŒ Wrong - data race possible:

unsafe { (*slot.data.get()).write(item); }

slot.sequence.store(new_seq, Ordering::Relaxed);  // Too weak!



// âœ… Correct - properly synchronized:

unsafe { (*slot.data.get()).write(item); }

slot.sequence.store(new_seq, Ordering::Release);  // Makes write visible


<pre><code>





<h3>Pitfall 3: Capacity Must Be Power-of-2




<pre><code>rust

// âŒ Wrong - will panic or perform poorly:

let queue = MpmcQueue::new(1000);  // Not power of 2!



// âœ… Correct - automatically rounded:

let queue = MpmcQueue::new(1000);  // Becomes 1024 internally

assert_eq!(queue.capacity(), 1024);


<pre><code>





<h2>ğŸ”§ Debugging and Profiling





<h3>Performance Profiling Tips




<pre><code>bash



<h1>CPU profiling with perf:

perf record -g --call-graph=dwarf target/release/examples/benchmark

perf report --stdio





<h1>Key metrics to watch:



<h1>- Cache miss rate (<5% is good)



<h1>- Branch prediction accuracy (>95% is good)  



<h1>- CPI (Cycles Per Instruction) (<2.0 is good)





<h1>Memory profiling with valgrind:

valgrind --tool=massif target/release/examples/benchmark





<h1>Look for:



<h1>- No memory leaks



<h1>- Reasonable peak memory usage



<h1>- No excessive allocations


<pre><code>





<h3>Common Performance Issues




<pre><code>rust

// Issue 1: Queue too small (frequent full condition)

let queue = MpmcQueue::new(16);  // Too small for high throughput

// Solution: Increase capacity to reduce contention



// Issue 2: Too many producers/consumers

// With >8 threads per operation, consider multiple queues

let queues: Vec<_> = (0..4).map(|_| MpmcQueue::new(256)).collect();



// Issue 3: Item size too large  

struct LargeItem([u8; 4096]);  // Each item = 4KB

// Solution: Use Arc<T> or Box<T> to store large items indirectly


<pre><code>



This deep dive reveals the sophisticated engineering behind the lockless MPMC queue, showing how careful attention to low-level details enables exceptional performance while maintaining safety guarantees.
</div></body></html>
